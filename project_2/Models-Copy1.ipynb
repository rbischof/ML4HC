{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Add, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTSNE(tsne_results, Y_test):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    label = Y_test\n",
    "    colors = ['blue', 'red']\n",
    "    plt.scatter(x=tsne_results[:,0], y=tsne_results[:,1], c=label, cmap=matplotlib.colors.ListedColormap(colors), s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_mean = np.asarray(pd.read_csv(\"data/diabetes/X_pca_mean.csv\", header=None))[:6000]\n",
    "X_val_pca_mean = np.asarray(pd.read_csv(\"data/diabetes/X_pca_mean.csv\", header=None))[6000:8000]\n",
    "X_test_pca_mean = np.asarray(pd.read_csv(\"data/diabetes/X_pca_mean.csv\", header=None))[8000:]\n",
    "\n",
    "X_lda_mean = np.asarray(pd.read_csv(\"data/diabetes/X_lda_mean.csv\", header=None))[:6000]\n",
    "X_val_lda_mean = np.asarray(pd.read_csv(\"data/diabetes/X_lda_mean.csv\", header=None))[6000:8000]\n",
    "X_test_lda_mean = np.asarray(pd.read_csv(\"data/diabetes/X_lda_mean.csv\", header=None))[8000:]\n",
    "\n",
    "X_pca_max = np.asarray(pd.read_csv(\"data/diabetes/X_pca_max.csv\", header=None))[:6000]\n",
    "X_val_pca_max = np.asarray(pd.read_csv(\"data/diabetes/X_pca_max.csv\", header=None))[6000:8000]\n",
    "X_test_pca_max = np.asarray(pd.read_csv(\"data/diabetes/X_pca_max.csv\", header=None))[8000:]\n",
    "\n",
    "X_lda_max = np.asarray(pd.read_csv(\"data/diabetes/X_lda_max.csv\", header=None))[:6000]\n",
    "X_val_lda_max = np.asarray(pd.read_csv(\"data/diabetes/X_lda_max.csv\", header=None))[6000:8000]\n",
    "X_test_lda_max = np.asarray(pd.read_csv(\"data/diabetes/X_lda_max.csv\", header=None))[8000:]\n",
    "\n",
    "X_pca_conc = np.asarray(pd.read_csv(\"data/diabetes/X_pca_conc.csv\", header=None))[:6000]\n",
    "X_val_pca_conc = np.asarray(pd.read_csv(\"data/diabetes/X_pca_conc.csv\", header=None))[6000:8000]\n",
    "X_test_pca_conc = np.asarray(pd.read_csv(\"data/diabetes/X_pca_conc.csv\", header=None))[8000:]\n",
    "\n",
    "X_lda_conc = np.asarray(pd.read_csv(\"data/diabetes/X_lda_conc.csv\", header=None))[:6000]\n",
    "X_val_lda_conc = np.asarray(pd.read_csv(\"data/diabetes/X_lda_conc.csv\", header=None))[6000:8000]\n",
    "X_test_lda_conc = np.asarray(pd.read_csv(\"data/diabetes/X_lda_conc.csv\", header=None))[8000:]\n",
    "\n",
    "Y = np.asarray(pd.read_csv(\"data/diabetes/Y.csv\", header=None)).reshape(-1).astype(int)\n",
    "Y_val = np.asarray(pd.read_csv(\"data/diabetes/Y_val.csv\", header=None)).reshape(-1).astype(int)\n",
    "Y_test = np.asarray(pd.read_csv(\"data/diabetes/Y_test.csv\", header=None)).reshape(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = np.hstack([Y, Y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Tree Classifier\n",
    "Use grid-search and cross-validation to find optimal parameters. Note that we merged the training and validation sets into X2 and Y2, since we do 5-fold cross-validation.<br>\n",
    "In this case we obtained the best results with the concatenated diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lda_conc\n",
    "X_val = X_val_lda_conc\n",
    "X_test = X_test_lda_conc\n",
    "X2 = np.vstack([X, X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'num_leaves': [11, 13, 14, 15],\n",
    "    'reg_alpha': [0.6, 0.7, 0.8, 0.9],\n",
    "    'min_data_in_leaf': [100, 110, 120, 130, 140, 150, 160, 170, 180],\n",
    "    'max_bin': [300, 350, 400, 450, 500],\n",
    "    'feature_fraction': [.2, .4, .5, .7, .8]\n",
    "    }\n",
    "\n",
    "lgb_estimator = lgbm.LGBMClassifier(objective='binary', \n",
    "                                    class_weight='balanced',\n",
    "                                    eval_metric='f1',\n",
    "                                    jobs=-1)\n",
    "gkf = KFold(n_splits=5, shuffle=True, random_state=42).split(X=X2_pca, y=Y2)\n",
    "gsearch = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=gkf, verbose=3, n_jobs=6, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lgb_model = gsearch.fit(X=X2, y=Y2)\n",
    "print(lgb_model.best_params_, lgb_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal classifier for LDA-encoded and concatenated diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafael\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "lgb_estimator = lgbm.LGBMClassifier(boosting_type='gbdt',\n",
    "                                    min_data_in_leaf=110, \\\n",
    "                                    num_leaves=15, \n",
    "                                    max_bin=350, \\\n",
    "                                    objective='binary', \\\n",
    "                                    class_weight='balanced',\n",
    "                                    feature_fraction=.41,\n",
    "                                    num_boost_round=1000,\n",
    "                                    reg_alpha=1.3\n",
    "                                   )\n",
    "lgb_estimator = lgb_estimator.fit(X, Y,\n",
    "                      eval_set=(X_val, Y_val), \n",
    "                      early_stopping_rounds=5, \n",
    "                      eval_metric='f1', verbose=0, \n",
    "                      callbacks = [lgbm.reset_parameter(learning_rate = np.linspace(0.12, 0.001, 20).tolist()+[0.001]*980)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   : 0.6365\n",
      "f1         : 0.5731062830299473\n",
      "AUROC      : 0.632878720285514\n",
      "prediction : [1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1]\n",
      "confidence : [6 5 7 6 6 6 6 7 5 6 5 5 6 6 6 5 7 5 6 6 6 5 6 6 6 5 6 6 7 6]\n",
      "true       : [0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "pred = lgb_estimator.predict(X_test).astype(int)\n",
    "print(\"accuracy   :\", accuracy_score(Y_test, pred))\n",
    "print(\"f1         :\", f1_score(Y_test, pred))\n",
    "print(\"AUROC      :\", roc_auc_score(Y_test, pred))\n",
    "print(\"prediction :\", pred[:30])\n",
    "print(\"confidence :\", (np.round(np.max(lgb_estimator.predict_proba(X_test)[:30], axis=1), 1)*10).astype(int))\n",
    "print(\"true       :\", Y_test[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this classifier only performs slightly better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the feature importances reveals that the features that were one-hot encoded were attributed the least importance in the dataset. In fact, they're completely ignored by the classifier (importance 0). <br>\n",
    "The most useful features are the number of inpatient, followed by the number of lab procedures. It is also worth noting that most of the features encoding the diagnoses (features 29 through 59) are among the important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(774, 20), (760, 15), (684, 7), (562, 17), (522, 21), (468, 38), (436, 3), (389, 2), (387, 49), (367, 30), (337, 0), (336, 40), (313, 5), (300, 36), (289, 31), (286, 39), (280, 54), (278, 34), (277, 47), (271, 10), (262, 26), (244, 41), (240, 18), (232, 6), (226, 13), (220, 52), (220, 37), (218, 14), (217, 32), (215, 56), (207, 33), (201, 59), (201, 4), (186, 57), (183, 51), (177, 42), (171, 44), (170, 58), (163, 19), (161, 35), (151, 28), (150, 50), (128, 16), (127, 46), (123, 45), (123, 29), (121, 48), (119, 55), (118, 53), (105, 43), (105, 27), (79, 25), (49, 1), (44, 23), (18, 22), (8, 8), (2, 24), (0, 12), (0, 11), (0, 9)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(zip(lgb_estimator.feature_importances_.tolist(), range(60))), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lda_mean\n",
    "X_val = X_val_lda_mean\n",
    "X_test = X_test_lda_mean\n",
    "X2 = np.vstack([X, X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 2, 5, 15, 30],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['auto'],\n",
    "    'shrinking': [True, False],\n",
    "    'decision_function_shape': ['ovo', 'ovr']\n",
    "    }\n",
    "\n",
    "clf = SVC(class_weight='balanced')\n",
    "\n",
    "gkf = KFold(n_splits=5, shuffle=True, random_state=42).split(X=X2, y=Y2)\n",
    "gsearch_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=gkf, verbose=3, n_jobs=-1, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = gsearch_clf.fit(X=X2, y=Y2)\n",
    "print(clf_model.best_params_, clf_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.05, break_ties=False, cache_size=200, class_weight='balanced',\n",
       "    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(class_weight='balanced', gamma='auto', C=0.05, kernel='rbf')\n",
    "clf.fit(X2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   : 0.6165\n",
      "f1         : 0.5764770844837106\n",
      "AUROC      : 0.6236617837728843\n",
      "prediction : [1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1]\n",
      "true       : [0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test).astype(int)\n",
    "print(\"accuracy   :\", accuracy_score(Y_test, pred))\n",
    "print(\"f1         :\", f1_score(Y_test, pred))\n",
    "print(\"AUROC      :\", roc_auc_score(Y_test, pred))\n",
    "print(\"prediction :\", pred[:30])\n",
    "print(\"true       :\", Y_test[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods like adaboost or knn did not even reach an f1-score of .5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lda_conc\n",
    "X_val = X_val_lda_conc\n",
    "X_test = X_test_lda_conc\n",
    "X2 = np.vstack([X, X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    x = Input(shape=(60,))\n",
    "\n",
    "    dense = Dense(100, activation='relu')(x)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    x2 = Add()([x, dense])\n",
    "    dense = Dense(100, activation='relu')(x2)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    x3 = Add()([x2, dense])\n",
    "    dense = Dense(100, activation='relu')(x3)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    x4 = Add()([dense, x3])\n",
    "    dense = Dropout(.1)(x4)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    dense = Dense(60, activation='relu')(dense)\n",
    "    x5 = Add()([dense, x4])\n",
    "    dense = Dense(100, activation='relu')(x5)\n",
    "    dense = Dense(60, activation='relu')(dense)   \n",
    "    dense = Dense(60, activation='relu')(dense)   \n",
    "    x6 = Add()([dense, x5])\n",
    "    dense = Dense(100, activation='relu')(x6)\n",
    "    dense = Dense(60, activation='relu')(dense)   \n",
    "    dense = Dense(60, activation='relu')(dense)   \n",
    "    x7 = Add()([dense, x6])\n",
    "    dense = Dense(100, activation='relu')(x7)\n",
    "    dense = Dense(60, activation='relu')(dense)    \n",
    "    dense = Dense(60, activation='relu')(dense)    \n",
    "    dense = Dropout(.1)(dense)\n",
    "    dense = Dense(100, activation='relu')(dense)\n",
    "    y = Dense(1, activation='sigmoid', name=\"y\")(dense)\n",
    "\n",
    "    model = models.Model(x, y, name=\"model\")\n",
    "    model.compile(optimizer='adagrad', loss=\"binary_crossentropy\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_223 (Dense)               (None, 100)          6100        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_224 (Dense)               (None, 60)           6060        dense_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_225 (Dense)               (None, 60)           3660        dense_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 60)           0           input_19[0][0]                   \n",
      "                                                                 dense_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_226 (Dense)               (None, 100)          6100        add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_227 (Dense)               (None, 60)           6060        dense_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_228 (Dense)               (None, 60)           3660        dense_227[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 60)           0           add_80[0][0]                     \n",
      "                                                                 dense_228[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_229 (Dense)               (None, 100)          6100        add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_230 (Dense)               (None, 60)           6060        dense_229[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_231 (Dense)               (None, 60)           3660        dense_230[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 60)           0           dense_231[0][0]                  \n",
      "                                                                 add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 60)           0           add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 60)           240         dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_232 (Dense)               (None, 100)          6100        batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_233 (Dense)               (None, 60)           6060        dense_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_234 (Dense)               (None, 60)           3660        dense_233[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 60)           0           dense_234[0][0]                  \n",
      "                                                                 add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_235 (Dense)               (None, 100)          6100        add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_236 (Dense)               (None, 60)           6060        dense_235[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_237 (Dense)               (None, 60)           3660        dense_236[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 60)           0           dense_237[0][0]                  \n",
      "                                                                 add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_238 (Dense)               (None, 100)          6100        add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_239 (Dense)               (None, 60)           6060        dense_238[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_240 (Dense)               (None, 60)           3660        dense_239[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 60)           0           dense_240[0][0]                  \n",
      "                                                                 add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_241 (Dense)               (None, 100)          6100        add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_242 (Dense)               (None, 60)           6060        dense_241[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_243 (Dense)               (None, 60)           3660        dense_242[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 60)           0           dense_243[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_244 (Dense)               (None, 100)          6100        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "y (Dense)                       (None, 1)            101         dense_244[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 117,181\n",
      "Trainable params: 117,061\n",
      "Non-trainable params: 120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 7s 1ms/sample - loss: 0.3254 - val_loss: 0.3196\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 4s 669us/sample - loss: 0.3119 - val_loss: 0.3152\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 5s 761us/sample - loss: 0.3044 - val_loss: 0.3136\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 4s 721us/sample - loss: 0.3001 - val_loss: 0.3138\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 4s 670us/sample - loss: 0.2962 - val_loss: 0.3132\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 4s 741us/sample - loss: 0.2944 - val_loss: 0.3133\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 5s 761us/sample - loss: 0.2917 - val_loss: 0.3138\n",
      "Epoch 8/100\n",
      "5952/6000 [============================>.] - ETA: 0s - loss: 0.2893\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6000/6000 [==============================] - 4s 721us/sample - loss: 0.2892 - val_loss: 0.3141\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 5s 863us/sample - loss: 0.2861 - val_loss: 0.3141\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 5s 836us/sample - loss: 0.2862 - val_loss: 0.3141\n",
      "Epoch 11/100\n",
      "5952/6000 [============================>.] - ETA: 0s - loss: 0.2873\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6000/6000 [==============================] - 4s 712us/sample - loss: 0.2868 - val_loss: 0.3142\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c17179790>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbs = []\n",
    "cbs.append(EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6, verbose=1))\n",
    "cbs.append(ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", patience=3, verbose=2))        \n",
    "m.fit(X, Y, shuffle=True, epochs=100, \\\n",
    "      validation_data=(X_val, Y_val), \\\n",
    "      callbacks=cbs, \n",
    "     class_weight={0:.4, 1:.6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy    : 0.6325\n",
      "f1          : 0.5668827342368886\n",
      "AUROC       : 0.6280508509106714\n",
      "prediction  : [1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1]\n",
      "non rounded : [8 7 2 8 3 3 4 2 6 3 4 3 8 3 3 4 4 7 4 8 5 6 4 4 6 3 4 6 7 6]\n",
      "true        : [0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "pred = m.predict(X_test).round().astype(int).reshape(-1)\n",
    "print(\"accuracy    :\", accuracy_score(Y_test, pred))\n",
    "print(\"f1          :\", f1_score(Y_test, pred))\n",
    "print(\"AUROC       :\", roc_auc_score(Y_test, pred))\n",
    "print(\"prediction  :\", pred[:30])\n",
    "print(\"non rounded :\", ((m.predict(X_test).round(1).reshape(-1))*10).astype(int)[:30])\n",
    "print(\"true        :\", Y_test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
